Multimodal Large Language Models(MLLMs)에 관한 survey와 모델 테스트를 다룰 예정입니다.

---

멀티모달이 가능해진 지금, AGI가 머지 않아 보입니다. 

사람이 언어로 말미암아 생각하는 것과 LLM이 사진과 언어를 연관지어 학습하는 방식은 매우 유사하다고 생각합니다. 다음의 이유로 그렇습니다. 

1. 수많은 `INPUT` 이후 언어를 구사할 수 있게 됨.
2. 언어를 구사하며, 품사의 개념을 모르더라도 품사에 맞게 사용할 수 있게 됨.
3. 품사나 단어의 의미 등 단어의 성질에 따라 단어를 치환할 수 있게 됨.
4. 하나의 언어를 구사할 수 있으면, 그와 유사한 다른 언어를 쉽게 배울 수 있음.
5. 말하면서(또는 쓰면서) 다음에 이어질 말을 생각하며, 이때 자주 쓰던 말을 위주로 쓰게 됨.

더 많은 이유가 있겠죠? 모델 실행과는 무관하더라도 모델이 어떻게 이렇게 돌아가는지 이해할 때는 유용한 것 같습니다. 

다시 돌아와서. MLLM은 텍스트, 이미지, 오디오와 비디오를 모두 임베딩하고 다룰 수 있는 멀티모달 모델로, 인풋과 아웃풋이 서로 다른 형식일 수 있으며, 
 인풋과 아웃풋이 모두 여러 modality일 수 있습니다. M"LLM"이라는 이름에서 알 수 있듯, 언어에 기반을 두고 있습니다. 인간이 언어로 말미암아 모든 생각을 전개하는 것과 비슷하죠?

MLLM의 사용 범위는 무궁무진합니다. OCR이 잡아내지 못하는 복잡한 손글씨를 잡아내거나, Bounding Box의 영역을 잡아주지 않아도 논문의 레이아웃을 인식하는 등
 이미지와 글이 필요한 많은 영역에서 좋은 성능을 보입니다. 

---

이 디렉토리에서는 다음의 컨텐츠를 다룹니다. 
1. [논문 리뷰]: MLLM 관련한 주요 논문을 리뷰합니다.
2. [구현]: 코드 레벨에서 모델을 구현해보고 그 성능을 측정합니다.
3. [Benchmark]: MLLM을 어떻게 평가할 것인지, 그 밴치마크를 조사합니다.
