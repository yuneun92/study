`Editor`: `윤은`

# **[📋논문 리뷰 1]** 
## Learning Transferable Visual Models From Natural Language Supervision
> - cs.CV 2022. [[논문]](https://arxiv.org/pdf/2204.06125), [[GITHUB]](https://github.com/openai/glide-text2im), [[DATASET]](https://github.com/openai/clip)
> - OpenAI
> - 26 Feb 2021

### Model Architecture
![image](https://github.com/yuneun92/study/assets/101092482/d813118e-0c5b-4640-b005-eae37ac4d5c3)

### Performance
![image](https://github.com/yuneun92/study/assets/101092482/303b898c-981c-413f-a0aa-206f54a7f40e)

![image](https://github.com/yuneun92/study/assets/101092482/5114a9ed-03e9-4151-b02d-8e1fa90af94c)

standard ImageNet 모델과의 비교

### Details

1. 기존 이미지 분류 모델과의 차이
- 기존 모델
  - 구동 방식: 이미지 → 임베딩 → 클래스마다의 확률을 구함 (e.g., 고양이, 강아지, 차, ...)
  - 문제점
    - 사진마다의 특수성이 살지 않음. 예를 들어 강아지의 사진은 강아지로 분류되기만 하면 되기 때문에, '강아지' 중에서 어떤 특성을 갖는지는 반영되지 않음.
- CLIP
  - 구동 방식 (모델 아키텍처 참고)
    - **이미지의 mini batch를 생성**, 그에 대한 텍스트도 **함께** 임베딩 (각 이미지와 텍스트에 대해 classification이 존재하는 꼴)
    - 기존에 이미지로부터 텍스트를 예측했던 것과 달리, 특정 이미지에 대해 **어떤 텍스트가 가장 유사도가 높은지**를 확인 (**contrastive objective**)
    - 충분히 큰 mini batch를 가지고 있어야 함. (전체 데이터셋의 크기에 근사)
  - 장점: 사진마다 specific한 디테일을 임베딩할 수 있게 됨. (데이터의 뉘앙스를 더 많이 담을 수 있다!)

**CLIP 모델에 대한 설명:**

**개요:**
CLIP (Contrastive Language-Image Pre-training)은 OpenAI에서 개발한 비전과 언어를 결합한 모델로, 이미지와 텍스트를 동시에 이해하고 처리할 수 있는 능력을 가지고 있습니다. 이 모델은 이미지와 텍스트 간의 상호작용을 학습하여 다양한 자연어와 이미지 처리 작업에서 뛰어난 성과를 보여주고 있습니다.

**작동 원리:**
1. **프레임워크:**
   - CLIP는 Vision Transformer (ViT) 및 Transformer 기반의 언어 모델을 기반으로 합니다.
   - 이미지와 텍스트를 처리하기 위해 Transformer 아키텍처가 통합적으로 사용됩니다.
   
2. **모델 학습:**
   - **비지도 사전 훈련:** 대규모 이미지 및 텍스트 데이터를 사용하여 사전 훈련됩니다.
   - **최소 지도 학습:** 텍스트와 이미지 쌍을 사용하여 레이블 없이 학습됩니다. 모델은 레이블이 없는 데이터에서도 텍스트 설명과 이미지 간의 상관 관계를 학습합니다.
   - **대규모 데이터셋 활용:** 수백만 개의 이미지와 텍스트 쌍을 사용하여 학습되며, 다양한 컨텍스트에서의 일반화 능력을 향상시킵니다.

3. **특징:**
   - **다목적 활용:** CLIP는 다양한 비전 및 언어 처리 작업에 유연하게 적용될 수 있습니다. 예를 들어, 이미지 검색, 이미지 분류, 텍스트 기반 이미지 생성 등에서 우수한 성능을 발휘합니다.
   - **상호 문맥 이해:** 이미지와 텍스트 간의 문맥을 이해하고 상호 작용하는 능력을 강화하여 더 정확한 결과를 도출합니다.
   - **상세한 텍스트-이미지 관계 학습:** 텍스트 설명과 이미지 간의 복잡한 상호작용을 학습하여 다양한 문맥에서 정확한 분류 및 예측을 가능하게 합니다.

4. **적용 사례:**
   - **이미지 분류:** 텍스트 설명을 바탕으로 이미지를 정확하게 분류합니다.
   - **이미지 검색:** 특정 텍스트 질의에 대해 관련 이미지를 검색하고 제공합니다.
   - **텍스트-이미지 생성:** 텍스트 입력에 대해 높은 품질의 이미지를 생성합니다.

CLIP 모델은 최근 몇 년간 비전과 언어를 결합한 AI 연구에서 중요한 발전을 이룩하며, 다양한 응용 분야에서 활용되고 있습니다.


## Hierarchical Text-Conditional Image Generation with CLIP Latents
> - cs.CV 2022. [[논문]](https://arxiv.org/pdf/2204.06125), [[GITHUB]](https://github.com/openai/glide-text2im), [[DATASET]](https://github.com/openai/clip)
> - OpenAI
> - 13 Apr 2022





---

# **[📋논문 리뷰 2]** 
## GLIDE : _Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models_

[논문](https://arxiv.org/pdf/2112.10741), [GITHUB](https://github.com/openai/glide-text2im), [DATASET](https://github.com/openai/clip) (CLIP의 데이터셋을 그대로 사용함.)

### 초록
CLIP guidance(DALL-E)와 classifier-free guidance(CLIP reranking)를 활용한 디퓨전 모델 테스트 결과, 인간 평가자가 느끼기에 후자가 캡션과 사진 간 유사돋가 높다고 평가함.

### INTRO
- 선행연구 분석
  - 의의: 자연어를 이용해 이미지를 편집함으로써 실제로 사용하기에 더 용이해짐 (사진의 미세한 조정 등)
  - 한계: 
    - 텍스트 조건부(text-conditional) 이미지 모델: 자유 형식의 텍스트 프롬프트에서 이미지를 합성할 수 있었으나 프롬프트에서 지시한 모든 것을 구현하지는 못합.
    - 조건이 없는(unconditional) 이미지 모델: 실제 사진과 구별할 수 없을 정도로 사실적인 이미지를 생성했음.
    
▶️ 이 둘을 조합하자!
  1. 자연어 설명을 조건으로 사용해, 텍스트 인코더를 사용하는 35억 파라미터 diffusion model을 만듦.
  2. 결과: CLIP과 classifier-free guidance를 비교
     - DALL-E보다 사실성 평가에서 87%, 캡션 유사성 평가에서 69% 더 선호되었음.
